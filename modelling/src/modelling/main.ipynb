{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19506c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Timestamp\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "import tensorflow as tf\n",
    "import dask.dataframe as dd\n",
    "import warnings                        # To ignore any warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e83641",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from prophet import Prophet\n",
    "from prophet.serialize import model_to_json\n",
    "import joblib\n",
    "from utils import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowModelTrainer:\n",
    "    def __init__(self, data_path:str) -> None:\n",
    "        logger.info(f\"Starting data preprocessing from file: {data_path}\")\n",
    "        logger.info(\"Loading data with Dask\")\n",
    "        self.data = dd.read_csv(data_path)\n",
    "        logger.info(f\"Loaded {len(self.data)} rows of data\")\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.models = {}\n",
    "\n",
    "    def prepocess_data(self):\n",
    "        logger.info(\"Converting datetime columns\")\n",
    "        self.data['StartedAt'] = dd.to_datetime(self.data['StartedAt'], utc=True)\n",
    "        self.data['CompletedAt'] = dd.to_datetime(self.data['CompletedAt'], utc=True)\n",
    "\n",
    "        logger.info(\"Calculating execution time\")\n",
    "        self.data[\"ExecutionTime\"] = (self.data['CompletedAt'] - self.data['StartedAt']).dt.total_seconds()\n",
    "\n",
    "        self.y_regression = self.data['ExecutionTime']\n",
    "        self.y_classification = (self.data['Conclusion'] == 'failure').astype(int)\n",
    "\n",
    "        logger.info(\"Encoding categorical variables\")\n",
    "        status_values = self.data['Status'].unique().compute()\n",
    "        logger.info(f\"Unique status values: {status_values}\")\n",
    "        status_map = {status: i for i, status in enumerate(status_values)}\n",
    "        self.data['StatusEncoded'] = self.data['Status'].map(status_map)\n",
    "\n",
    "        logger.info(\"Encoding Success/Failure\")\n",
    "        self.data['SuccessEncoded'] = (self.data['Conclusion'] == 'failure').astype(int)\n",
    "\n",
    "        self.X = self.data[['StatusEncoded', 'ExecutionTime']]\n",
    "        self.y = (self.data['Conclusion'] == 'success').astype(int)\n",
    "\n",
    "        logger.info(\"Sorting data by StartedAt\")\n",
    "        self.sequence_data = self.data.sort_values('StartedAt')[['StatusEncoded', 'ExecutionTime', 'SuccessEncoded']]\n",
    "\n",
    "        logger.info(\"Computing final DataFrame\")\n",
    "        self.sequence_data_pd = self.sequence_data.compute()\n",
    "        logger.info(f\"Columns in computed sequence data: {self.sequence_data_pd.columns}\")\n",
    "\n",
    "        # self.time_series_data = self.data.groupby('StartedAt').size().reset_index()\n",
    "        # self.time_series_data.columns = ['ds', 'y']\n",
    "        logger.info(f\"Preprocessing complete. Final shape: {self.sequence_data.shape}\")\n",
    "        logger.info(\"Preprocesing complete\")\n",
    "\n",
    "    def prepare_sequence(self, sequence_length=10):\n",
    "        logger.info(f\"Preparing sequences with length: {sequence_length}\")\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for i in range(len(self.sequence_data_pd) - sequence_length):\n",
    "            X.append(self.sequence_data_pd.iloc[i:i+sequence_length][['StatusEncoded', 'ExecutionTime']].values)\n",
    "            y.append(self.sequence_data_pd.iloc[i+sequence_length]['SuccessEncoded'])\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        logger.info(f\"Created sequences. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        logger.info(\"Normalizing ExecutionTime\")\n",
    "        X[:,:,1] = (X[:,:,1] - np.mean(X[:,:,1])) / np.std(X[:,:,1])\n",
    "\n",
    "        logger.info(\"Splitting data into train and test sets\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        logger.info(f\"Train shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
    "        logger.info(f\"Test shapes - X: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    # def train_random_forest(self):\n",
    "    #     logger.info(\"Training Random Forest Model\")\n",
    "    #     X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "    #     rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    #     rf_model.fit(X_train, y_train)\n",
    "    #\n",
    "    #     y_pred = rf_model.predict(X_test)\n",
    "    #     accuracy = accuracy_score(y_test, y_pred)\n",
    "    #     # print('Random Forest Model: ')\n",
    "    #     # print(f\"Accuracy: {accuracy}\")\n",
    "    #     logger.info(f\"Accuracy: {accuracy}\")\n",
    "    #\n",
    "    #     self.models['random_forest'] = rf_model\n",
    "\n",
    "    # def train_regression_model(self):\n",
    "    #     logger.info(\"Training Regression\")\n",
    "    #     X_train, X_test, y_train, y_test = train_test_split(self.X, self.y_regression, test_size=0.2, random_state=42)\n",
    "    #\n",
    "    #     regression_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    #     regression_model.fit(X_train, y_train)\n",
    "    #\n",
    "    #     # Evaluate model\n",
    "    #     y_pred = regression_model.predict(X_test)\n",
    "    #     mse = mean_squared_error(y_test, y_pred)\n",
    "    #     # print(f\"Mean Squared Error: {mse}\")\n",
    "    #     logger.info(\"The Mean Squared Error: {}\".format(mse))\n",
    "    #\n",
    "    #     # Cross-validation\n",
    "    #     cv_scores = cross_val_score(regression_model, self.X, self.y_regression, cv=5)\n",
    "    #     logger.info(f\"Cross-validation scores: {cv_scores}\")\n",
    "    #     logger.info(f\"Mean CV score: {np.mean(cv_scores)}\")\n",
    "    #     logger.info(\"-------------------------------\")\n",
    "    #     logger.info(\"End Regression\")\n",
    "    #     # print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    #     # print(f\"Mean CV score: {np.mean(cv_scores)}\")\n",
    "    #\n",
    "    #     self.models['regression'] = regression_model\n",
    "\n",
    "    def train_rnn(self):\n",
    "        logger.info(\"Start RNN\")\n",
    "        X_train, X_test, y_train, y_test = self.prepare_sequence()\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            LSTM(32),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "        logger.info(f\"Test accuracy: {test_accuracy:.4f} - Test loss: {test_loss:.4f}\")\n",
    "        logger.info(\"-------------------------------\")\n",
    "        logger.info(\"End RNN\")\n",
    "        self.models['rnn'] = model\n",
    "\n",
    "    # def train_time_series(self):\n",
    "    #     logger.info(\"Start Prophet\")\n",
    "    #     model = Prophet()\n",
    "    #     model.fit(self.time_series_data)\n",
    "    #\n",
    "    #     future = model.make_future_dataframe(periods=30)\n",
    "    #     forecast = model.predict(future)\n",
    "    #\n",
    "    #     logger.info(\"Time Series Forecasting Model (Prophet) trained successfully.\")\n",
    "    #     logger.info(\"-------------------------------\")\n",
    "    #     logger.info(\"End Prophet\")\n",
    "    #     self.models['prophet'] = model\n",
    "\n",
    "    def save_models(self, base_path: str):\n",
    "        joblib.dump(self.models['random_forest'], f\"{base_path}_rf.joblib\")\n",
    "        joblib.dump(self.models['regression'], f\"{base_path}_regression.joblib\")\n",
    "        self.models['rnn'].save(f\"{base_path}_rnn.keras\")\n",
    "        # with open(f\"{base_path}_prophet.json\", \"w\") as fout:\n",
    "        #     fout.write(model_to_json(self.models['prophet']))\n",
    "\n",
    "    def train_all_models(self):\n",
    "        logger.info(\"Start training\")\n",
    "        self.prepocess_data()\n",
    "        # self.train_random_forest()\n",
    "        self.train_rnn()\n",
    "        # self.train_time_series()\n",
    "        # self.train_regression_model()\n",
    "        logger.info(\"-------------------------------\")\n",
    "        logger.info(\"Models Succesfully Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45993caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = WorkflowModelTrainer('all_steps.csv')\n",
    "logger = log(path=\"\", file=\"training.logs\")\n",
    "trainer = WorkflowModelTrainer('/Users/davidayomide/Downloads/Dev/FINALPROJ/action-monitoring-template/modelling/MINE/all_steps.csv')\n",
    "trainer.train_all_models()\n",
    "trainer.save_models('workflow_models')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
